---
title: "A Bayesian, Frequentist, and Machine Learner meet at a bar..."
author: Dror Berel
format: 
  closeread-html:
    css: "index.css"
    remove-header-space: true
    debug-mode: false
    cr-style:
      narrative-text-color-sidebar: black
      narrative-font-family: 'Georgia, "Times New Roman", Times, serif'
      narrative-font-size: 1.25rem
      narrative-sidebar-width: minmax(400px, 1fr)
      narrative-border-radius: 5px
      narrative-background-color-overlay: "#A7D8FF"
      narrative-background-color-sidebar: "#A7D8FF"
      section-background-color: white
      poem-font-family: 'Georgia, "Times New Roman", Times, serif'
knitr:
  opts_chunk: 
    dev.args:
      bg: transparent
number-sections: false
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

# The Two (or Three) Cultures of Statistics

::::: {.cr-section layout="overlay-right"}
On 2001, in a paper called 'Statistical Modeling, the Two Cultures',
<a href="https://senate.universityofcalifornia.edu/_files/inmemoriam/html/leobreiman.htm" style="color:white;">
Leo Breiman</a> termed two cultures [^1].[@cr-paper]

"There are two cultures in the use of statistical modeling to reach
conclusions from data. One, **Data Modeling**, assumes that the data are
generated by a given stochastic data model.\
The other, **Machine Learning** (ML) uses algorithmic models and treats
the data mechanism as unknown".\@cr-paper

A third approach, **Bayesian** statistics, is not mentioned in Breiman's
paper.\
Loosely defined, "**Bayesian** statistics is a theory in the field of
statistics based on the Bayesian interpretation of probability, where
probability expresses a degree of belief in an event" [^2]. @cr-bayes

::: {#cr-paper}
![](paper_orig.png){height="50%"}
:::

::: {#cr-bayes .scale-to-fill}
![](bayes.png)
:::
:::::

[^1]: Statistical Modeling: The Two Cultures Author(s): Leo Breiman
    Source: Statistical Science, Vol. 16, No. 3 (Aug., 2001), pp.
    199-215 <https://www2.math.uu.se/~thulin/mm/breiman.pdf>

[^2]: <https://en.wikipedia.org/wiki/Bayesian_statistics>

::: section-header
<h1 style="margin: revert;">

Key Principals For Each Approach

</h1>
:::

:::::: cr-section
**Frequentist / Data Modeling** @cr-F_txt

**Machine Learning** @cr-ML_txt

**Bayesian** @cr-B_txt

::: {#cr-ML_txt}
-   Key Principles:
    -   Focuses on building models that generalize well to unseen data,
        often prioritizing predictive performance over interpretability.
    -   Primarily data-driven, relying on patterns in the data rather
        than explicit probabilistic assumptions.
    -   Uses techniques like optimization, cross-validation, and
        regularization to minimize prediction error.\
    -   Includes supervised, unsupervised, and reinforcement learning
        approaches.
-   Strengths:\
    -   Excels at tasks with large, complex datasets (e.g., image
        recognition, natural language processing).\
    -   Flexible and capable of modeling highly non-linear
        relationships.\
    -   Emphasis on real-world predictive accuracy.\
-   Weaknesses:\
    -   Models can lack interpretability (e.g., black-box nature of deep
        learning).\
    -   Requires large datasets to perform well.\
    -   May overfit or fail to generalize if not carefully regularized
        or validated.\
-   Example Use Cases:\
    -   Fraud detection, recommendation systems, autonomous driving, and
        more.\
    -   Algorithms like decision trees, neural networks, and support
        vector machines.
:::

::: {#cr-F_txt}
-   Key Principles:
    -   Focuses on long-run frequencies of events and emphasizes
        hypothesis testing and confidence intervals.
    -   Probabilities are interpreted as the long-term frequency of
        events. Parameters are fixed, unknown quantities.
    -   Relies on data to compute sample-based estimates and test
        hypotheses.
-   Strengths:
    -   Simple and well-established frameworks for hypothesis testing
        (e.g., p-values, t-tests).
    -   Does not require prior assumptions (like priors in Bayesian
        statistics).
-   Weaknesses:
    -   Interpretation of results (e.g., p-values) can be unintuitive.
    -   Less flexible when dealing with small datasets or complex
        models.
-   Example Use Cases:
    -   Traditional hypothesis testing in experimental science.
    -   Classical methods like ANOVA, regression, and chi-squared tests.
:::

::: {#cr-B_txt}
-   Key Principles:\
    -   Combines prior knowledge with observed data to update beliefs
        about parameters using Bayes' theorem.\
    -   Probabilities represent degrees of belief or uncertainty about
        events.\
    -   Parameters are treated as random variables with probability
        distributions.\
    -   Prior distributions reflect existing knowledge or assumptions.\
-   Strengths:\
    -   Provides a natural framework for updating beliefs as new data is
        observed.\
    -   Can handle small datasets effectively when combined with
        informative priors.\
    -   Results are often easier to interpret probabilistically (e.g.,
        "the probability of the parameter being within a certain
        range").\
-   Weaknesses:\
    -   Requires specification of priors, which can be subjective.\
    -   Computationally intensive, especially for complex models.\
-   Example Use Cases:\
    -   Predictive modeling in medicine (e.g., estimating patient
        risk).\
    -   Hierarchical modeling and small-sample inference.\
    -   Applications in fields requiring uncertainty quantification,
        like robotics or weather forecasting.
:::
::::::

::: {.cr-section .centered layout="overlay-center"}
In summary,\
\
**Frequentist Statistics**\
Best for traditional inferential tasks.\
\
**Bayesian Statistics**\
Best for uncertainty and small-sample problems.\
\
**Machine Learning**\
Best for predictive and large-scale applications.
:::

| Aspect | **Frequentist** | **Bayesian** | **Machine Learning** |
|------------------|------------------|------------------|------------------|
| Focus | Hypothesis testing | Probabilistic inference | Predictive performance |
| Probability | Long-run frequencies | Degree of belief | Often implicit or not used |
| Parameters | Fixed and unknown | Random variables | May be implicit or non-parametric |
| Prior Knowledge | Not used | Explicitly incorporated | Not typically used |
| Dataset Size | Works with moderate to large datasets | Can handle small datasets with priors | Requires large datasets for best results |
| Interpretability | High | Moderate | Often low |
| Applications | Traditional science | Fields needing uncertainty quantification | Industry, big data applications |

: Key Differences {.striped .hover .table}

<p>Back to Breiman's two cultures paper...</p>

::: section-header
```{=html}
<h1 style="margin: revert;"> 1. Can 'Data Modeling' culture be considered as Frequentist? </h1>
```
:::

::::: cr-section
The **data modeling** culture described by Leo Breiman is not strictly
synonymous with **frequentist** methods, but it aligns closely with many
principles of **frequentist** statistics. Here’s why:@cr-Freq_Model_txt

::: {#cr-Freq_Model_txt}
-   **Assumptions and Parameter Estimation**: Data modeling culture
    often relies on parametric models like linear regression or
    generalized linear models, which are staples in frequentist
    methodology. These models estimate parameters based on observed
    data, with a focus on likelihood functions and hypothesis
    testing—core concepts in frequentist inference.

-   **Model Interpretability**: Like traditional frequentist approaches,
    the data modeling culture emphasizes interpretability and providing
    clear, theoretically grounded explanations of relationships within
    the data.

-   **Role of Probability**: Frequentist statistics typically view
    probabilities as long-run frequencies of events, which aligns with
    the reliance on fixed, pre-specified models in the data modeling
    culture. Probabilities here are tied to specific distributions
    assumed to represent the data-generation process.

-   **Contrast to Bayesian Methods**: Bayesian approaches, which
    incorporate prior beliefs into the model and update these beliefs
    with observed data, are less central to the traditional data model
    culture. This difference reinforces the association of the culture
    with frequentist perspectives, although Bayesian approaches can be
    used within data modeling paradigms in certain contexts.\
:::

Nuance in Data Modeling and Frequentist comparison: @cr-Nuance_txt

::: {#cr-Nuance_txt}
-   **Overlap but Not Identity**:\
    While data modeling and frequentist methods often overlap, they are
    not identical. For instance, data modeling is defined more by the
    approach of assuming a probabilistic structure for the data and its
    focus on model-based inference than by strict adherence to
    frequentist philosophies.

-   **Emerging Perspectives**:\
    Some modern extensions of data modeling, such as penalized
    regression (e.g., Lasso, ridge), use computational techniques that
    are not strictly frequentist but can still align with the
    interpretive goals of the data modeling culture.\
:::
:::::

::: {.cr-section layout="overlay-center"}
In summary, while the **data modeling** culture aligns strongly with
many aspects of **frequentist** methods, it is better understood as a
subset or close associate rather than being fully synonymous. It
reflects the **frequentist** mindset in its reliance on fixed models,
assumptions, and hypothesis testing.
:::

::: section-header
```{=html}
<h1 style="margin: revert;"> 2. Why Breiman did not include Bayesian statistics in his 2001 paper? </h1>
```
:::

::::: cr-section
Breiman's 2001 paper primarily aimed to critique the dominance of the
data modeling culture and to introduce the algorithmic modeling culture
as a more effective paradigm for handling modern, complex data analysis
challenges.\
The omission of Bayesian statistics likely occurred for a few key
reasons related to the paper's focus and the state of statistical
practice **at that time** @cr-why_not_bayes

::: {#cr-why_not_bayes}
-   **Focus on Paradigm Shift Between Two Dominant Cultures**: Breiman's
    primary argument was to contrast the data modeling culture,
    characterized by traditional parametric models and interpretability,
    with the algorithmic modeling culture, represented by machine
    learning techniques that emphasize predictive accuracy and
    data-driven models. Including Bayesian methods would have introduced
    a third lens, which might have diluted the clarity of his
    two-culture argument.

-   **Bayesian Methods' Marginal Role in Practical Applications
    (2001)**: At the time Breiman wrote his paper, Bayesian statistics,
    though well-established theoretically, had not achieved widespread
    adoption in many practical or industrial data analysis settings.
    Computational limitations and a lack of accessible software hindered
    the popularity of Bayesian methods compared to frequentist
    approaches and emerging machine learning techniques.

-   **Focus on Assumption-Driven Models**: Breiman's critique of the
    data modeling culture hinges on its reliance on rigid assumptions
    about data-generating processes. Bayesian statistics, while offering
    flexibility through prior distributions, also operates within
    model-based frameworks. Thus, Breiman might have considered it part
    of, or closely aligned with, the data modeling culture, even if not
    explicitly mentioned.

-   **Bayesian Statistics Doesn't Align with Algorithmic Modeling**:
    Breiman was advocating for algorithmic modeling—methods that rely
    heavily on data-driven approaches and often do not assume an
    explicit probabilistic model. Bayesian approaches still heavily
    involve modeling (e.g., specifying priors and likelihoods) and thus
    may not align well with the algorithmic philosophy.

-   **Paper’s Argument as a Provocation**: Breiman likely structured his
    paper as a sharp, provocative dichotomy to spark debate within the
    statistics and machine learning communities. Adding more nuanced
    considerations, like Bayesian methods, might have complicated the
    narrative.
:::

Contemporary Perspectives: @cr-contemporary

::: {#cr-contemporary}
-   In modern data science, Bayesian methods have gained significant
    traction, aided by advancements in computational techniques (e.g.
    Markov Chain Monte Carlo) and software tools like Stan and PyMC.

-   Bayesian frameworks now offer powerful solutions that can bridge
    elements of the two cultures, suggesting that Breiman’s omission may
    reflect the context of his time rather than the relevance of
    Bayesian approaches to today’s data science landscape.
:::
:::::

::::: cr-section
@cr-bar \# Still confused? What if... A Bayesian, Frequentist, and
Machine Learner practitioners meet at a bar. What happened next none of
them could have predicted...

::: {#cr-bar}
![](bar_60.png)
:::

::: {#cr-stack}
![](stack.png)
:::

@cr-stack

A fictional conversation between the three practitioners was found on
[stats.stackexchange](https://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning)[^3]
:::::

::: {.cr-section layout="overlay-center"}
Keep scrolling down for a conversation between a Bayesian (B),
Frequentist (F), and Machine Learner (ML).

Skip it if you are not interested: @sec-about_this_page (The skit
includes 65 sentences! It's okay to change your mind.)
:::

:::::: {.cr-section layout="overlay-left"}
::: {#cr-ML .scale-to-fill}
![](rabbit_20.png)
:::

::: {#cr-F .scale-to-fill}
![](dog_20.png)
:::

::: {#cr-B .scale-to-fill}
![](cat_20.png)
:::

```{r, rtext, include=FALSE}
library(tidyverse)

tags_vec <- c(
  '{pan-to="25%,-50%"}',
  '{scale-by="1.5"}',
  #'{zoom-to="3"}'
  '{pan-to="18%,-15%" scale-by="1.5"}'
)

out_collapsed <- readr::read_delim('./CL_text.txt') %>% 
  setNames(c('Character', 'text')) %>% 
  mutate(text = text %>% str_remove_all(' \"|\"')) %>% 
  mutate(tag_rand = sample(tags_vec, nrow(.), replace = TRUE)) %>% 
  mutate(Character_tag = str_c('[@cr-', Character, ']',tag_rand), .before = 1) %>% 
  mutate(Character = str_c(Character, ':')) %>% 
  unite(out, c('Character', 'text', 'Character_tag'), sep = ' ') %>% 
  pull(out) %>% 
  str_c(collapse = '\n \n')

# CL_text %>% count(Character)
# CL_text$text[[1]]
```

```{r}
#| results: asis
cat(out_collapsed)
```

```{r, eval = FALSE}
#| results: asis
cat('
F: asis1 [@cr-F]{pan-to="18%,-15%" scale-by="2.4"}

ML: asis2 @cr-B
'
)
```
::::::

:::: {.cr-section layout="overlay-center"}
::: {#cr-text}
```{r, rtext}
#| echo: true
#| message: false
#| warning: false
```
:::

# About this page: {#sec-about_this_page}

Congratulations! You made it to the end of the page.\
This page was created using [Quarto](https://quarto.org/),
[Closeread](https://closeread.dev/) and [R](https://www.r-project.org/).
The conversation section was built from a raw text file with 64
sentences (rows)!\
Standard Closeread tags consist of tagging each section with a \@
cr-tags for each 'frame' in the "story".\
A 'frame' is like a Power-Point slide, but with vertical scrolling, so
slides (Frames) are automatically changed while you scroll down/up.\
Since I was not going to "Copy -\> Paste" 64 closeread \@ cr-tags, I
leveraged Quarto's programmatic solution to run things iteratively.

The trick was first to combine each sentence from the text with the cr-
tag[@cr-text] , print it as a long string, and then, in
meta-programming, embed it in the Quarto document with a **#\| results:
asis** chunk option.

Most of the technical content in this blog was inspired by responses
from large language models (LLMs) to questions that had been puzzling
me.\
The primary educational goal of this post is to showcase how tools like
Quarto and Closeread can be leveraged for meta-programming.\
Additionally, it highlights that in the current age of LLMs (as of
2024), expertise in a specific domain is not necessarily a prerequisite
for sharing valuable insights.\
Instead, knowing how to ask the right questions and employing effective
storytelling can transform complex technical topics into accessible,
engaging resources for a broader, less technical audience.
::::

::::: cr-section
::: {focus-on="cr-about" pan-to="0%,0%" scale-by="1.2"}
About me:
:::

::: {#cr-about}
Dror Berel\
**Statistician** **Specialization in Causal Inference and Machine
Learning.**\
**R programmer**\
**Solving business needs**\
[Personal page](https://drorberel.github.io)\
[LinkedIn](https://www.linkedin.com/in/dror-berel/)\
[Blog](https://drorberel.medium.com)
:::
:::::

[^3]: [Aaron
    NcdDid](https://stats.stackexchange.com/users/7817/aaron-mcdaid)
